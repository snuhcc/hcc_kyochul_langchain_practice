


from dotenv import load_dotenv
import os

load_dotenv(".env", verbose=True)
LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')
HCC_OPENAI_API_KEY = os.getenv('HCC_OPENAI_API_KEY')
os.environ['OPENAI_API_KEY'] = HCC_OPENAI_API_KEY


LANGCHAIN_TRACING_V2=True
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_API_KEY=LANGCHAIN_API_KEY
LANGCHAIN_PROJECT="kyochul"


import bs4
from langchain import hub
from langchain.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma, FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate






loader = PyPDFLoader('./parliamentary_records/국회회의록_22대_416회_3차_농림축산식품해양수산위원회 (1) (1).pdf')
docs = loader.load()
print(f'len(docs): {len(docs)}')
docs


docs[1].__dict__





text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200)

splits = text_splitter.split_documents(docs)





embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)





retriever = vectorstore.as_retriever()





template = "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Just output the concise short-form answering without any additional explanation.\nQuestion: {question} \nContext: {context} \nAnswer:"

prompt = PromptTemplate.from_template(template)
prompt





llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


retriever | format_docs





rag_chain = (
    {'context': retriever | format_docs, 'question': RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)





questions = [
    '2023년에 농협이 특별 상여금으로 직원들에게 얼마를 줬어?',
    '마사회에서 온라인 발매를 도입할 당시에 가장 큰 걱정이 뭐야?',
    '강호동이 생각하는 전국 하나로마트가 소비자가격을 낮추는 대신에 할인행사를 하는 이유가 뭐야?',
    '강준석이 생각하는 내년 2월부터 부산항이 직기항이 아니고 셔틀로 이용되는데, 셔틀로 운송될 때는 어떤 장단점이 있어?',
    '농협법 제159조의2가에 어떤거에 대한 정의가 나와있어?',
    '강호동은 몇만t의 쌀 소비가 목표야?',
    '마사회가 청렴도 종합평가에서 몇년만에 2등급을 받았어?',
    '농업렵동조합이 정부에 1조 6천억을 반납한 기간이 몇년부터 몇년까지야?',
    '생축장의 목적이 뭐야?',
    '소위원회에서 농림축산식품법안심사 위원은 몇명이야?'
]


answers = [
    '4100억',
    '불법도박',
    '과잉생산 문제 해결 및 경영적인 측면의 문제 해결',
    '15일 단축',
    '농업 지원비 부과에 대한 명확한 정의',
    '목표가 딱히 안정해져 있음',
    '굉장히 오랜만임(정확한 기간은 안나와있음)',
    '2009년부터 16년 사이',
    '한우 번식 사업을 통해서 양질의 송아지를 생산하고 저렴한 가격으로 농가에 분양하는 목적',
    '11명'
]


responses = []
for q in questions:
    responses.append(rag_chain.invoke(q))





# 결과 출력
# print(f"URL: {url}")
print(f"문서의 수: {len(docs)}")
print("===" * 20)
for i in range(10):
    print(f'question: {questions[i]}, \nresponse: {responses[i]}, \nanswer: {answers[i]}')
    print()





from langchain.chains import LLMChain
from langchain.chains import SequentialChain
from langchain_community.llms import OpenAI


prompt2 = "Please rate how similar are they between 1)'{response}' and 2)'{answer}'. If you think that 1) and 2) are conveying the same meaning, give 100. However, if the information they contain are not the same at all, then give 0. Just give the score without containing any other explanation."
prompt2 = PromptTemplate.from_template(prompt2)


chain1 = LLMChain(llm=llm, prompt=prompt, output_key='response', verbose=True)
chain2 = LLMChain(llm=llm, prompt=prompt2, verbose=True)


chain = SequentialChain(chains=[chain1, chain2], input_variables=['context', 'question', 'answer'], verbose=True)


ans = chain.run({'context': 'Kyochul is 27 years old', 'question': 'how old is Kyochul?', 'answer': '27'})


ans


# 결과 출력
# print(f"URL: {url}")
print(f"문서의 수: {len(docs)}")
print("===" * 20)
for i in range(10):
    print(f'question: {questions[i]}, \nresponse: {responses[i]}, \nanswer: {answers[i]}')
    ans = chain.run({'context': {, 'question': 'how old is Kyochul?', 'answer': '27'})
    print()


for idx, q in enumerate(questions):
    # response = rag_chain.invoke(q)
    
    ans = chain.run({'context': retriever | format_docs, 'question': q, 'answer': answers[idx]})
    print(ans)



